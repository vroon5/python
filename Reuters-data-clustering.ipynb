{"cells":[{"cell_type":"code","source":["%python"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#Amazon S3 Integration\n\nACCESS_KEY = ############\nSECRET_KEY = ################################\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"dataminingoutput\"\nMOUNT_NAME = \"mount2\"\n\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["### data import ###\n\ndataRDD = sc.textFile(\"/FileStore/tables/5dqc4cpq1481007336149/newdata.txt\").map(lambda line: line.split(\"\\t\")).map(lambda (a,b): b).zipWithIndex().map(lambda (a,b): (b,a))\ndataRDD.take(2)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["### cleaning, tokenization and stopwords removal ###\n\nimport re\nstopwords = sc.textFile(\"/FileStore/tables/u69wlumy1480742599455/stopwords_en.txt\")\nstop = stopwords.collect()\n\nsplit_regex = r'\\W+'\n\ndef tokenize(string):\n  return [x for x in filter(lambda s: len(s) > 0, re.split(split_regex,string.lower())) if x not in set(stop)]\n\ntokenized = dataRDD.map(lambda (a,b): (a,tokenize(b))).cache()\ntokenized.take(2)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["### Stemming ###\n\nimport nltk\nfrom nltk.stem.porter import *\nstemmer = PorterStemmer()\n\ndef stem(list):\n  return [stemmer.stem(word) for word in list]\n\nstemmed = tokenized.map(lambda (a,b): (a,stem(b))).cache()\nstemmed.take(2)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["### tf and tfidf function definitions ###\nimport math\nfrom operator import add\n\ndef tf(tokens):\n  tfDict = dict()\n  for i in tokens:\n    tfDict[i] = tfDict.setdefault(i, 0) + 1\n  factor=1.0/sum(tfDict.values())\n  for k in tfDict:\n\t    tfDict[k]=tfDict[k]*factor\n  return tfDict\n\ndef idfs(corpus):\n    N = corpus.count()\n    def f(x): return x \n    def g(x): return len(x)\n    uniqueTokens = corpus.flatMapValues(f).distinct().map(lambda (a,b): b)\n    tokenSumPairTuple = uniqueTokens.map(lambda x: (x,1)).reduceByKey(add)\n    return tokenSumPairTuple.map(lambda (a,b): (a,math.log(N/b)))\n  \ndef tfidf(tokens, idfs):\n    tfDict = tf(tokens)\n    tfIdfDict = { k: tfDict.get(k, 0) * idfs.get(k, 0) for k in set(tfDict)  & set(idfs) }\n    return tfIdfDict"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["### create the tf-tdf matrix and convert it to 2D numpy array (to input into clustering algorithms) ###\n\nimport pandas as pd\nfrom numpy import *\n\nidfsRDD = idfs(stemmed).cache()\nidfsMap = idfsRDD.collectAsMap()\n\ntokenslist = stemmed.collect()\n\n#create list of tf-idf dictionaries\ntfidfarray = []\nfor index,elem in enumerate(tokenslist):\n    tfidfarray.append(tfidf(elem[1],idfsMap))\ntfidfarray\n\n#convert this list to numpy array and replace all NaNs with zeros\ndf = pd.DataFrame(tfidfarray)\n\ndataArray = df.as_matrix()\nwhere_are_NaNs = isnan(dataArray)\ndataArray[where_are_NaNs] = 0\ndataArray.shape"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["### PCA ### \nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot\n\n#SCREE PLOT\npca = PCA(n_components=50)\npca.fit(dataArray)\n\nev = pca.explained_variance_\nnum = [i for i in range(len(ev))]\n\nfig, ax = pyplot.subplots()\nax.plot(num,ev)\ndisplay(fig)\n\n#PCA - with optimum number of components\npca2 = PCA(n_components=5)\npca2.fit(dataArray)\ndataArray2 = pca2.fit_transform(dataArray)\ndataArray2.shape\n\n#PCA - 2 components for plotting\npca3 = PCA(n_components=2)\npca3.fit(dataArray)\ndataArray3 = pca3.fit_transform(dataArray)\ndataArray3.shape"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["### FUZZY C MEANS FUNCTION ###\nimport numpy as np\n#euclidean distance function\ndef dist(x,y):   \n    return np.sqrt(np.sum((x-y)**2))\n\n#Parameters:    \n# data -  data in the form of numpy array array (transposed tabular data)\n# c - no of clusters\n# m - fuzzy coefficient\n# req_iter -  # iterations required, tol= error tolerance\n\ndef fuzzyc(data,c,m,req_iter,tol):\n    #Fuzzy coefficient\n    m=m\n\n    #Cluster centers\n    c=c\n\n    #length of the dataset\n    #l=len(data)\n    l=len(data[1])\n\n    # the power for the next membership funciton calc.\n    exp=2/(m-1)\n\n    #dimensions of the data\n    dim=len(data)\n\n    #no of iterations\n    req_iter=req_iter\n\n    #minimization tolerance\n    tol=tol\n\n\n    #random number generation for the initial membership matrix\n    e = np.random.random((c,l))\n    randtot=np.sum(e, axis=0)\n\n    #new Uik calc.\n    Uik=np.divide(e,randtot)\n    Uikm=(Uik)**m\n\n    # Loop for minimization function\n    iterationcnt=0\n    d=(dim,c)\n    njmin=1\n    jmin=0\n    \n    while (np.absolute(njmin-jmin)>tol and iterationcnt<req_iter):      \n        #################### Iteration a ######################\n        #center calc.\n        nvi=np.zeros(d)\n        for i in range(dim):\n            nvi[i]=np.sum(np.multiply(data[i],Uikm),axis=1)/np.sum(Uikm,axis=1)\n\n        #new membership matrix calc.\n        dataT=data.T\n        nviT=nvi.T\n        size=(c,l)\n        nUik=np.zeros(size)\n        for j in range(c):\n            for i in range(l):    \n                xk_vi_dist=dist(dataT[i],nviT[j])\n                denom=0\n                for r in range(c):\n                        xk_vr_dist=dist(dataT[i],nviT[r])\n                        ratio=(xk_vi_dist/xk_vr_dist)**exp\n                        denom=denom+ratio\n        #new Uik is calc. here           \n                nUik[j,i]=1/denom\n\n        #new Uikm is calc. here        \n        nUikm=(nUik)**m\n\n        #minimization function calc.\n        njm=np.zeros(size)\n        for j in range(c):\n            for i in range(len(dataT)):    \n                njm[j,i]=(dist(dataT[i],nviT[j]))**2\n\n        njmin=np.sum(np.multiply(njm,nUikm))\n\n        #################### Iteration b ######################\n        #center calc.\n        vi=np.zeros(d)\n        for i in range(dim):\n            vi[i]=np.sum(np.multiply(data[i],nUikm),axis=1)/np.sum(nUikm,axis=1)\n\n        #new membership matrix calc.\n        dataT=data.T\n        viT=vi.T\n        size=(c,l)\n        Uik=np.zeros(size)\n        for j in range(c):\n            for i in range(l):    \n                xk_vi_dist=dist(dataT[i],viT[j])\n                denom=0\n                for r in range(c):\n                        xk_vr_dist=dist(dataT[i],viT[r])\n                        ratio=(xk_vi_dist/xk_vr_dist)**exp\n                        denom=denom+ratio\n        #new Uik is calc. here           \n                Uik[j,i]=1/denom\n\n        #new Uikm is calc. here        \n        Uikm=(Uik)**m\n\n        #minimization function calc.\n        jm=np.zeros(size)\n        for j in range(c):\n            for i in range(len(dataT)):    \n                jm[j,i]=(dist(dataT[i],viT[j]))**2\n\n        jmin=np.sum(np.multiply(jm,Uikm))\n        iterationcnt=iterationcnt+1\n    ####################################################################################################\n\n    #this will have the final centers\n    newcenters=viT\n\n    #This will have the allocation of datapoints to centers\n    allocation=np.argmax(Uik.T,axis=1)\n    \n    #This will have SSE\n    SSE=0\n    for i in range(l):\n        SSE=SSE+dist(newcenters[allocation[i]],data.T[i])\n    \n    #This will have SST\n    SST=0\n    for i in range(l):\n        SST=SST+dist(data.T[i],np.mean(newcenters,0))\n        \n    #This will have the membership matix U\n    mem_mat=Uik.T\n    \n    #FPC : varies from (1/c to 1)\n    fpc=np.sum(Uik**2)/l\n\n    #FuPC (normalized to 0 to 1)\n    fpc_norm=(np.sum(Uik**2)/l-(1/c))/(1-(1/c))\n    \n    \n    print(\"The vectors below in the order are\")\n    print(\"Allocation,centers,SSE,SST,mem_mat,fpc,fpc_norm,iterationcount\")\n    return allocation+1,newcenters,SSE,SST,mem_mat,fpc,fpc_norm,iterationcnt"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["### RUNNING THE ALGORITHM FOR THE REDUCED DATA ###\n\nresult = fuzzyc(dataArray2.T,8,2,1000,0.001)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["### plotting the results ### \n\nfig, ax = pyplot.subplots()\nax.scatter(dataArray3[:,0], dataArray3[:,1], c=result[0])\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["### off the shelf fuzzy C means algorithm for comparison ###\nimport skfuzzy as fuzz\n\ncntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(\n        dataArray2.T, 8, 2, error=0.001, maxiter=1000, init=None)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#K means clustering\nfrom scipy import cluster\nfrom matplotlib import pyplot\n\ninitial = cluster.vq.kmeans(dataArray2,8)\ncent, var = initial\nassignment,cdist = cluster.vq.vq(dataArray2,cent)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["### Intercluster distances for K means, fuzzy c means and off the shelf sk-fuzzy ###\n\nkmeansintercluster = []\nfor j in range(len(cent)):\n  kmeansintercluster.append([dist(cent[j],cent[i]) for i in range(len(cent))])\n\nskfuzzyintercluster = []\nfor j in range(len(cntr)):\n  skfuzzyintercluster.append([dist(cntr[j],cntr[i]) for i in range(len(cntr))])\n\nfuzzyintercluster = []\nfor j in range(len(result[1])):\n  fuzzyintercluster.append([dist(result[1][j],result[1][i]) for i in range(len(result[1]))])"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["### EXPORTING FILES TO ANALYZE RESULTS ###\n\ndef tf2(tokens):\n  tfDict = dict()\n  for i in tokens:\n    tfDict[i] = tfDict.setdefault(i, 0) + 1\n  #factor=1.0/sum(tfDict.values())\n  #for k in tfDict:\n\t#    tfDict[k]=tfDict[k]*factor\n  return tfDict\n\nfrom pyspark.sql import SQLContext\nsqlCtx = SQLContext(sc)\nMOUNT_NAME = \"mount2\"\n\n#dataArray3\nsqlCtx.createDataFrame(pd.DataFrame(dataArray3)).repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/mnt/%s/reuters/dataArray3\" % MOUNT_NAME)\n\n#dataArray2\nsqlCtx.createDataFrame(pd.DataFrame(dataArray2)).repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/mnt/%s/reuters/dataArray2\" % MOUNT_NAME)\n\n#allocation\nsqlCtx.createDataFrame(pd.DataFrame(result[0])).repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/mnt/%s/reuters/allocation\" % MOUNT_NAME)\n\n#mmat\nsqlCtx.createDataFrame(pd.DataFrame(result[4])).repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/mnt/%s/reuters/mmat\" % MOUNT_NAME)\n\n#tdidf\nsqlCtx.createDataFrame(df).repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/mnt/%s/reuters/tfidf\" % MOUNT_NAME)\n\n#just tfs, not tf-idfs\ntfarray = []\nfor index,elem in enumerate(tokenslist):\n    tfarray.append(tf2(elem[1]))\nlen(tfarray)\n\ntf = pd.DataFrame(tfarray)\ntf = tf.loc[:, ((tf.isnull().sum(axis=0)) <= 1550)]\ntf = tf.fillna(0)\ntf.shape\n#tf.loc[1,:]\n\n#tf\nsqlCtx.createDataFrame(tf).repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/mnt/%s/reuters/tf\" % MOUNT_NAME)\n\n#mmat_skfuzzy\nsqlCtx.createDataFrame(pd.DataFrame(u.T)).repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/mnt/%s/reuters/mmat_skfuzzy\" % MOUNT_NAME)\n\n#cntr_skfuzzy\nsqlCtx.createDataFrame(pd.DataFrame(cntr)).repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/mnt/%s/reuters/cntr_skfuzzy\" % MOUNT_NAME)\n\n#centers\nsqlCtx.createDataFrame(pd.DataFrame(result[1])).repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/mnt/%s/reuters/center\" % MOUNT_NAME)\n\n#Kmeans allocation\nsqlCtx.createDataFrame(pd.DataFrame(assignment)).repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/mnt/%s/reuters/kmeans_allocation\" % MOUNT_NAME)\n\n#Kmeans centroids\nsqlCtx.createDataFrame(pd.DataFrame(cent)).repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/mnt/%s/reuters/kmeans_centroid\" % MOUNT_NAME)\n\n#Kmeans intercluster dist\nsqlCtx.createDataFrame(pd.DataFrame(kmeansintercluster)).repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/mnt/%s/reuters/kmeans_intercluster\" % MOUNT_NAME)\n\n#skfuzzy intercluster dist\nsqlCtx.createDataFrame(pd.DataFrame(skfuzzyintercluster)).repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/mnt/%s/reuters/skfuzzy_intercluster\" % MOUNT_NAME)\n\n#fuzzy interclster dist\nsqlCtx.createDataFrame(pd.DataFrame(fuzzyintercluster)).repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/mnt/%s/reuters/fuzzy_intercluster\" % MOUNT_NAME)"],"metadata":{},"outputs":[],"execution_count":15}],"metadata":{"name":"Reuters-data-clustering","notebookId":2181854168399871},"nbformat":4,"nbformat_minor":0}
